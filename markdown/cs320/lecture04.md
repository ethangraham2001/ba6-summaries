# Lecture 04: Parsers

## Formal Grammars

Intuitively, a set of rewriting rules *(aka production rules)*. For instance

- $S \rightarrow Tb$
- $T \rightarrow \varepsilon$
- $T \rightarrow aT$

Rules are composed of terminal and non-terminal symbols, written in lower case
and upper case respectively.

Starting from a start symbol $S$ we can recursively apply rules until obtaining
a word consisting only of terminal symbols. A sequence of rule applications is
called a derivation.

Formally, a formal grammer $G = (\Sigma, \Theta, S, P)$ where

- $\Sigma$ alphabet (a finite set of terminal symbols)
- $\Theta$ set of non-terminal symbols s.t. $\Sigma \cap \Theta = \emptyset$
- $S \in \Theta$ denotes a start symbol
- $P$ is a finite set of production rules $x \rightarrow y$

The language generated by grammar $G$ is written $L(G)$

## Chomsky Hierarchy

> The Chomsky hierarchy (infrequently referred to as the Chomsky–Schützenberger 
> hierarchy) in the fields of formal language theory, computer science, and 
> linguistics, is a containment hierarchy of classes of formal grammars. A 
> formal grammar describes how to form strings from a language's vocabulary 
> (or alphabet) that are valid according to the language's syntax. Linguist 
> Noam Chomsky theorized that four different classes of formal grammars existed 
> that could generate increasingly complex languages. Each class can also 
> completely generate the language of all inferior classes (set inclusive). 

### Type 0: Unrestricted Grammars

No constraints on the production rules. They generate exactly all languages 
that can be recognized by a Turing machine.

An example would be

$$
L = \lbrace w | w \textit{ describes a terminating Turing machine } \rbrace
$$

Which is an undecidable problem. Thus deciding whether a word $w$ belongs to
$L(G)$ of an unrestricted grammar $G$ is indecidable.

### Type 1: Context-sensitive Grammars

Productions are restricted to either of the following forms

- $T \rightarrow \varepsilon$ 
- $xTz \rightarrow xyz$ with $x,y,z$ strings of terminal symbols, and $y$ 
non-empty

Intuitively, the second case only allows us to replace $T$ when it is 
surrounded by some context. Deciding whether $L(G) = \emptyset$ for an arbitrary
context-sensitive grammar $G$ is undecidable.

Think of XML, where a starting tag `<a>` needs to be matched by a `<\a>` for
whatever is contained between them to be considered valid. Thus it is indeed
context sensitive.

### Type 2: Context-free Grammars

Production rules are restricted to

- $T \rightarrow y$ where $y$ is a non-empty word of terminal symbols.

Given two context-free grammars $G_1, G_2$ deciding whether $L(G_1) = L(G_2)$
is undecidable.

#### Pushdown Automaton

A finite-state automaton that uses a stack to keep state of the input it has
processes so far.

Pushdown automata, contrary to DFA or NFA, can recognize context-free grammars.

- For any PDA, there exists a CFG that generates the same language recognized by
it
- For any CFG, there exists a PDA that recognizes the same language that it
generates.

This equivalence is known as the Chomsky-Schützenberger theorem.

The stack is just there for keeping internal state of our program - we can do
whatever we want to with it.


The stack is just there for keeping internal state of our program - we can do
whatever we want to with it.
#### Greibach Normal Form

This is when the right-hand side of all production rules start with a terminal
symbol, optionally followed by some variables. More precisely, all production
rules are of the form

$$
A \rightarrow a A_1 A_2 \dots A_n
$$

This ensures that every transition rule is either terminal, or ends only in
non-terminal symbols.

Every context free grammar can be transformed into an equivalent Greibach 
normal form.

Given a grammar in GNF and a derivable string in the grammar with length $n$,
any top-down parser will halt at depth $n$.

### Type 3: Regular Grammars

Productions rules limited to the following forms

- $T \rightarrow \varepsilon$
- $T \rightarrow \varepsilon$ 
- $T \rightarrow sU$

These languages are recognizable by finite state automaton.

## Regular Expression (Regex)

Character string describing a regular language. They can be parsed very 
efficiently. Formally, given an alphabet $\Sigma$, regexes are recursively 
defined as followed:

- $\emptyset$ is a regex s.t. $L(\emptyset) = \emptyset$
- $\varepsilon$ is a regex s.t. $L(\varepsilon) = \lbrace \varepsilon \rbrace$
- if $s \in \Sigma$ then $s$ is a regex s.t. $L(s) = \lbrace s \rbrace$
- if $r_1, r_2$ are regexes, then $r_1 | r_2$ is a regex s.t.
$L(r_1|r_2) = L(r_1) \cup L(r_2)$
- if $r_1, r_2$ are regexes, then $r_1 r_2$ is a regex s.t.
$L(r_1|r_2) = L(r_1) \cdot L(r_2)$
- if $r$ is a regex, then $r^\star$ is a regex s.t. $L(r^\star) = L(r)^\star$

### Examples

- $abc$ recognizes $\lbrace abc \rbrace$
- $a^\star b$ recognizes $\lbrace a, ab, aab, aaab, \dots \rbrace$
- $(ab)^\star | c^\star$ recognizes 
$\lbrace \varepsilon, ab abab, ababab, \dots c, cc, ccc, \dots \rbrace$
- $((ab)|c)^\star$ recognizes 
$\lbrace \varepsilon ab, c, abab, abc, cab, cc, \dots \rbrace$

# Re-Chomsky Hierarchy

Type 3 and type 2 languages are desirable because they can be parsed 
efficiently in $O(n)$ and $O(n^3)$ respectively in general.

In many cases *(for example indendation in python/Scala)* we end up with type-1
context-sensitive languages. This becomes quite complex.

## Parse Trees

We can represent the result of a derivation as a tree whose branch nodes are
non-terminal symbols, and whose leaf nodes are terminal symbols.

Grammars can be ambiguous, i.e. they are capable of generating different parse
trees from the same input $\rightarrow$ parse forests, eventually deciding on
one of them. In general, disambiguation is preferable.

Determining whether a grammar is ambiguous is undecidable, but there exist
semi-decidable algorithms for context-free grammars. Some languages are
inherently ambiguous, and there exist parsing algorithms designed to handle
such cases.

## Left and Right Recursion

We can transform production rules to eliminate left or right recursion. 
A production rule is left (right) recursive if it features a non-terminal symbol
on the left (right) such that we can apply the rule recursively.

$S \rightarrow Sa | b$ is left-recursive, and $S \rightarrow aS | b$ is 
right-recursive. We note that left or right recursion may also involve some
indirection between rules *(i.e. won't be directly recursive, but will be after
applying multiple rules)*.

To eliminate left or right recus

- Add a new non-terminal symbol $S'$ at the end (left-recursive) or start
(right-recursive) of non-recursive production rules.
- The production rules for $S'$ swap the position of the recursion.

## Left and Right Factoring

Happens when two production rules have the same prefix or suffix. This is a 
problem because many grammars naturally exhibit situations where common 
prefixes occur. It is convenient for some parsing algorithms to process symbols
from left to right *(or conversely)* and select the rule to apply immediately!

- top-down parsers suffer from left-factoring
- bottom-up parses suffer from right-factoring

**Example:** $S \rightarrow aT | aTbS | T$ suffers from left-factoring.

Fortunately, we can offer factor out common prefixes/suffixes in a separate
production rule to "delay" decisions depending on prefixes / suffixes. The
above example becomes

- $S \rightarrow aTU | T$
- $U \rightarrow bS | \varepsilon$

## Parsing Algorithms

### LR Parsing *(left-to-right, rightmost derivation)*

Given a stack of input predictions and an input to parse, LR parser repeatedly
does either of the following:

- **Shift:** push the input's head onto the stack
- **Reduce:** replace elements at the top of the stack by a non-terminal symbol

We use different policies for deciding when we should reduce. For example
longest-match, shortest-match, etc...

## Parser Combinators

Intuitively a parser is a function

$$
p: \, \Sigma^\star \rightarrow R \times \Sigma^\star
$$

where 

- $\Sigma$ is an alphabet
- $R$ is the parser's output *(a parse tree)*

We take the input, and return a parse tree + the remaining part of the word we
haven't parsed yet.

Since functions compose, we can compose parsers

$$
f(p_1, p_2)(w) = (r_1, r_2), w_2
$$

where $p_1(2) = (r_1, w_1) \wedge p_2(w_1) = (r_2, w_2)$

Scala example:

```scala
// `S` is symbol type, `s` and `e` are the start and end positions
// `R` is the result
trait Parser[S, R]:
    def apply(w: Array[S], s: Int, e: Int): (R, Int)
```

Here is an example of an actual implementation

```scala
final class One[S](letter: S) extends Parser[S, S]:
    def parse(w: Array[S], s: Int, e: Int) =
        if w(s) == letter then (letter, s + 1) else
            throw IllegalArgumentException()

val s = "aab".codePoints().toArray
val a = One('a': Int)
```

We can extend this as follows:

```scala
extension[S, R1] (self: Parser[S, R1])
    // concatenation of parsers
    infix def ++ [R2](other: Parser[S, R2]) = new Parser[S, (R1, R2)]:
        def apply(w: Array[S], s: Int, e: Int) =
            val (r1, e1) = self(w, s, e)
            val (r2, e2) = other(w, e1, e) // picks up where the previous left off
            ((r1, r2), e2)

val s = "aab".codePoints().toArray
val a = One('a': Int)
val b = One('b': Int)
println((a ++ a ++ b)(s, 0, s.length)) // (((97, 97), 98), 3)
```

Here is another parser combinator:

```scala
extension[S, R1] (self: Parser[S, R1])
    // parses the union of languages (symbol is in one or the other)
    infix def || [R2](other: Parser[S, R2]) = new Parser[S, R1 | R2]:
        def apply(w: Array[S], s: Int, e: Int) =
            try self(w, s, e) catch _ => other(w, s, e)

val s = "aab".codePoints().toArray
val a = One('a': Int)
val b = One('b': Int)
// this next line uses backtracking. Tries to parse a++b union with a
println(((a ++ b) || a)(s, 0, s.length)) // (97, 1)
```

- `a++b` fails to parse `"aab"`
- `a` parses it, and returns `('a', 1)` which will be outputted, by definition
of the union of parsers. Thus we have only parsed the first symbol.

